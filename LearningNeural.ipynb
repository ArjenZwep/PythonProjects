{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LearningNeural",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "PB2tRYeX7DmF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "#import the needed library's. \n",
        "#for now because we use a build in dataset for learning ANN we only need tensorflow\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uYOHdc3I7Xp_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#import the dataset, it's a dataset with fashion pictures of different clothes from zalando\n",
        "(x_train, y_train), (x_val, y_val) = keras.datasets.fashion_mnist.load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_lEAsOaT-fgX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Changing the range of a pixel from 0-255 to 0-1, for more accurate predictions\n",
        "def preprocess(x, y):\n",
        "  x = tf.cast(x, tf.float32) / 255\n",
        "  y = tf.cast(y, tf.int64)\n",
        "  return x, y\n",
        "\n",
        "#Create the data and process it into readable data for our model, through hot encoding\n",
        "def create_dataset(xs, ys, n_classes=10):\n",
        "  ys = tf.one_hot(ys, depth=n_classes)\n",
        "  return tf.data.Dataset.from_tensor_slices((xs, ys)) \\\n",
        "    .map(preprocess) \\\n",
        "    .batch(128)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6y00e7x_vfi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Making a trainingset\n",
        "train_dataset = create_dataset(x_train, y_train)\n",
        "val_dataset = create_dataset(x_val, y_val)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07we9D8wK_QV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Building the shell of ANN model\n",
        "model = keras.Sequential([\n",
        "                          #The shape needs to be 28 * 28 because the pictures are 28 x 28 pixels\n",
        "                          keras.layers.Reshape(target_shape=(28 * 28,), input_shape=(28, 28)),\n",
        "                          keras.layers.Dense(units=256, activation='relu'),\n",
        "                          keras.layers.Dense(units=192, activation='relu'),\n",
        "                          keras.layers.Dense(units=128, activation='relu'),\n",
        "                          #We end with 10 units because we have 10 catogery's that can be predicted\n",
        "                          keras.layers.Dense(units=10, activation='softmax'),\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7aNduUhDmF51",
        "colab_type": "code",
        "outputId": "85e25c14-2ddc-42a9-a39f-f9c070d6e309",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 477
        }
      },
      "source": [
        "#We train the model with the adam algorithim, it's a catogarization model which we value on accuracy\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "#Training the model 10 times\n",
        "history = model.fit(\n",
        "    train_dataset.repeat(),\n",
        "    epochs=10,\n",
        "    steps_per_epoch=500,\n",
        "    validation_data=val_dataset.repeat(),\n",
        "    validation_steps=2\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Expected a shuffled dataset but input dataset `x` is not shuffled. Please invoke `shuffle()` on input dataset.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Train on 500 steps, validate on 2 steps\n",
            "Epoch 1/10\n",
            "500/500 [==============================] - 5s 11ms/step - loss: 0.0861 - acc: 0.9656 - val_loss: 0.0573 - val_acc: 0.9777\n",
            "Epoch 2/10\n",
            "500/500 [==============================] - 5s 11ms/step - loss: 0.0641 - acc: 0.9744 - val_loss: 0.0603 - val_acc: 0.9793\n",
            "Epoch 3/10\n",
            "500/500 [==============================] - 5s 11ms/step - loss: 0.0579 - acc: 0.9768 - val_loss: 0.0581 - val_acc: 0.9738\n",
            "Epoch 4/10\n",
            "500/500 [==============================] - 5s 11ms/step - loss: 0.0537 - acc: 0.9784 - val_loss: 0.0606 - val_acc: 0.9734\n",
            "Epoch 5/10\n",
            "500/500 [==============================] - 5s 11ms/step - loss: 0.0508 - acc: 0.9797 - val_loss: 0.0601 - val_acc: 0.9781\n",
            "Epoch 6/10\n",
            "500/500 [==============================] - 5s 10ms/step - loss: 0.0479 - acc: 0.9808 - val_loss: 0.0543 - val_acc: 0.9797\n",
            "Epoch 7/10\n",
            "500/500 [==============================] - 5s 10ms/step - loss: 0.0454 - acc: 0.9817 - val_loss: 0.0598 - val_acc: 0.9762\n",
            "Epoch 8/10\n",
            "500/500 [==============================] - 5s 10ms/step - loss: 0.0438 - acc: 0.9825 - val_loss: 0.0670 - val_acc: 0.9762\n",
            "Epoch 9/10\n",
            "500/500 [==============================] - 5s 11ms/step - loss: 0.0417 - acc: 0.9832 - val_loss: 0.0728 - val_acc: 0.9773\n",
            "Epoch 10/10\n",
            "500/500 [==============================] - 5s 10ms/step - loss: 0.0393 - acc: 0.9841 - val_loss: 0.0623 - val_acc: 0.9766\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iA1uMaGD9VxU",
        "colab_type": "text"
      },
      "source": [
        "An accurracy of 0.9841! That is insane"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lNPUb99K1KtV",
        "colab_type": "code",
        "outputId": "77f619d9-995c-433e-ac7e-4c383c9d7cb9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "#Using our model\n",
        "predictions = model.predict(val_dataset)\n",
        "#Looking at one prediction made\n",
        "predictions[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([2.1342560e-06, 1.2172507e-06, 2.0374462e-06, 9.3207930e-08,\n",
              "       1.7764729e-05, 1.8003030e-04, 2.1049641e-06, 4.1520648e-02,\n",
              "       4.5431489e-07, 9.5827359e-01], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    }
  ]
}